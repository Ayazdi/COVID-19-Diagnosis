{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COVID = './dataset/covid/'\n",
    "PATH_NORMAL = './dataset/normal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_list = os.listdir(PATH_COVID)\n",
    "normal_list = os.listdir(PATH_NORMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reshape_img(image):\n",
    "    img = load_img(image, target_size=(224, 224))\n",
    "    x = img_to_array(img)/255.\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 2/70 [01:04<36:43, 32.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f721e823c302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                               \u001b[0msave_to_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH_COVID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                               \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'covid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                               save_format='jpeg'):        \n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     format=self.save_format)\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mbatch_x_miscs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_misc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         output = (batch_x if batch_x_miscs == []\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2136\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m                 \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "                                    rescale=1./255,\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    brightness_range=[0.2,1.0],\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=[0.5,1.0],\n",
    "                                    fill_mode='nearest')\n",
    "\n",
    "for xr in tqdm.tqdm(covid_list):\n",
    "    X = np.array([load_reshape_img(PATH_COVID + image) for image in covid_list])\n",
    "    X = X.reshape(X.shape[0], 224, 224, 3)    \n",
    "    i = 0\n",
    "    for batch in datagen.flow(X, batch_size=70,\n",
    "                              save_to_dir=PATH_COVID, \n",
    "                              save_prefix='covid', \n",
    "                              save_format='jpeg'):        \n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xr in tqdm.tqdm(normal_list):\n",
    "    X = np.array([load_reshape_img(PATH_NORMAL + image) for image in normal_list])\n",
    "    X = X.reshape(X.shape[0], 224, 224, 3)    \n",
    "    i = 0\n",
    "    for batch in datagen.flow(X, batch_size=70,\n",
    "                              save_to_dir=PATH_NORMAL, \n",
    "                              save_prefix='normal', \n",
    "                              save_format='jpeg'):        \n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v3 = InceptionV3(weights='imagenet', include_top=False, classes=2, input_shape=(224, 224, 3))\n",
    "new_layers = model_v3.output\n",
    "\n",
    "new_layers = GlobalAveragePooling2D()(new_layers)\n",
    "\n",
    "new_layers = Dense(128, activation='relu')(new_layers)\n",
    "new_layers = Dropout(0.5)(new_layers)\n",
    "new_layers = BatchNormalization()(new_layers)\n",
    "\n",
    "new_layers = Dense(2, activation='softmax')(new_layers)\n",
    "model_v3 = Model(inputs=model_v3.inputs, outputs=new_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the first 51 layers\n",
    "for layer in model_v3.layers[:52]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v3.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='sparse_categorical_crossentropy', metrics=['accuracy']) #Stochastic gradient descent optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28106 images belonging to 2 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`validation_steps` should not be specified if `validation_data` is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-04c4413a6551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     epochs = 5)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    614\u001b[0m           distribution_strategy=distribution_strategy)\n\u001b[0;32m    615\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m       raise ValueError('`validation_steps` should not be specified if '\n\u001b[0m\u001b[0;32m    617\u001b[0m                        '`validation_data` is None.')\n\u001b[0;32m    618\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_adapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_adapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `validation_steps` should not be specified if `validation_data` is None."
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "#                                     rescale=1./255,\n",
    "#                                     rotation_range=40,\n",
    "#                                     width_shift_range=0.2,\n",
    "#                                     height_shift_range=0.2,\n",
    "#                                     brightness_range=[0.2,1.0],\n",
    "#                                     shear_range=0.2,\n",
    "#                                     zoom_range=[0.5,1.0],\n",
    "#                                     fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './dataset/train',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#         './dataset/valid/', # same directory as training data\n",
    "#         target_size=(224, 224),\n",
    "#         batch_size=1,\n",
    "#         class_mode='binary'\n",
    "#   ) # set as validation data\n",
    "\n",
    "\n",
    "\n",
    "# model_v3.fit(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=100,\n",
    "#         epochs=2)\n",
    "model_v3.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 60,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = 10,\n",
    "    epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1872 images belonging to 2 classes.\n",
      "Found 467 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 55s 471ms/step - loss: 0.9342 - accuracy: 0.5272 - val_loss: 0.4808 - val_accuracy: 0.8254\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 54s 459ms/step - loss: 0.7547 - accuracy: 0.6052 - val_loss: 0.4218 - val_accuracy: 0.8728\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 54s 459ms/step - loss: 0.6156 - accuracy: 0.6880 - val_loss: 0.3483 - val_accuracy: 0.9181\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 53s 457ms/step - loss: 0.5303 - accuracy: 0.7420 - val_loss: 0.2575 - val_accuracy: 0.9440\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 54s 458ms/step - loss: 0.4567 - accuracy: 0.7842 - val_loss: 0.4157 - val_accuracy: 0.8470\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 54s 457ms/step - loss: 0.3822 - accuracy: 0.8504 - val_loss: 0.2286 - val_accuracy: 0.9418\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 54s 458ms/step - loss: 0.3362 - accuracy: 0.8718 - val_loss: 0.1044 - val_accuracy: 0.9849\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 54s 461ms/step - loss: 0.2942 - accuracy: 0.9065 - val_loss: 0.2299 - val_accuracy: 0.9203\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 53s 456ms/step - loss: 0.2686 - accuracy: 0.9188 - val_loss: 0.1680 - val_accuracy: 0.9720\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 54s 460ms/step - loss: 0.2435 - accuracy: 0.9338 - val_loss: 0.1616 - val_accuracy: 0.9677\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 53s 457ms/step - loss: 0.2183 - accuracy: 0.9546 - val_loss: 0.1068 - val_accuracy: 0.9806\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 53s 456ms/step - loss: 0.1943 - accuracy: 0.9594 - val_loss: 0.1327 - val_accuracy: 0.9698\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 54s 463ms/step - loss: 0.1898 - accuracy: 0.9653 - val_loss: 0.0935 - val_accuracy: 0.9892\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 54s 460ms/step - loss: 0.1649 - accuracy: 0.9754 - val_loss: 0.1226 - val_accuracy: 0.9784\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 54s 461ms/step - loss: 0.1544 - accuracy: 0.9776 - val_loss: 0.1318 - val_accuracy: 0.9763\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 54s 460ms/step - loss: 0.1434 - accuracy: 0.9786 - val_loss: 0.1762 - val_accuracy: 0.9547\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 54s 460ms/step - loss: 0.1412 - accuracy: 0.9829 - val_loss: 0.1450 - val_accuracy: 0.9655\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 54s 461ms/step - loss: 0.1214 - accuracy: 0.9877 - val_loss: 0.2534 - val_accuracy: 0.9440\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 54s 458ms/step - loss: 0.1128 - accuracy: 0.9877 - val_loss: 0.1491 - val_accuracy: 0.9698\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 54s 460ms/step - loss: 0.1081 - accuracy: 0.9877 - val_loss: 0.3170 - val_accuracy: 0.9440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab8ccabb00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                                    rescale=1./255,\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    brightness_range=[0.2,1.0],\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=[0.5,1.0],\n",
    "                                    fill_mode='nearest',\n",
    "                                    validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './dataset/',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        subset='training')\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        './dataset/', # same directory as training data\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        subset='validation') # set as validation data\n",
    "\n",
    "model_v3.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // 16,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // 16,\n",
    "    epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "def save_model(model):\n",
    "    # serialize model to JSON\n",
    "    with open(f\"{model}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_v3.to_json())\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model_v3.save_weights(f\"{model}.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "save_model('model')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
